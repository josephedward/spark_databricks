{"cells":[{"cell_type":"code","source":["myCollection = \"Spark The Definitive Guide : Big Data Processing Made Simple\"\\\n  .split(\" \")\nwords = spark.sparkContext.parallelize(myCollection, 2)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35afe330-b9b1-4109-b6ed-9d3c57e97967"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nwords.map(lambda word: (word.lower(), 1))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de556832-d685-4f49-8d92-07ffa2b41d08"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: PythonRDD[1] at RDD at PythonRDD.scala:58</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: PythonRDD[1] at RDD at PythonRDD.scala:58</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["keyBy\n- specify function that creates the key from your current value\n\n\nIn this case, you are keying by the first letter in the word. Spark stores the record as the value for the keyed RDD:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"202313de-8f00-4933-bca1-dfd03d439a8a"}}},{"cell_type":"code","source":["keyword = words.keyBy(lambda word: word.lower()[0])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d31b29c-9803-4543-92ef-ffacb5e9dc36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["mapValues\n- you have a set of key–value pairs, you can begin manipulating them as such\n- If a tuple, Spark will assume that the first element is the key, and the second is the value\n- can explicitly choose to map-over the values (and ignore the individual keys)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfd9c843-c4ad-4f49-95b8-632f26ebf147"}}},{"cell_type":"code","source":["\nkeyword.mapValues(lambda word: word.upper()).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b117d842-5d4c-4d7b-bd6d-a2ec7f1d9e42"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[4]: [(&#39;s&#39;, &#39;SPARK&#39;),\n (&#39;t&#39;, &#39;THE&#39;),\n (&#39;d&#39;, &#39;DEFINITIVE&#39;),\n (&#39;g&#39;, &#39;GUIDE&#39;),\n (&#39;:&#39;, &#39;:&#39;),\n (&#39;b&#39;, &#39;BIG&#39;),\n (&#39;d&#39;, &#39;DATA&#39;),\n (&#39;p&#39;, &#39;PROCESSING&#39;),\n (&#39;m&#39;, &#39;MADE&#39;),\n (&#39;s&#39;, &#39;SIMPLE&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [(&#39;s&#39;, &#39;SPARK&#39;),\n (&#39;t&#39;, &#39;THE&#39;),\n (&#39;d&#39;, &#39;DEFINITIVE&#39;),\n (&#39;g&#39;, &#39;GUIDE&#39;),\n (&#39;:&#39;, &#39;:&#39;),\n (&#39;b&#39;, &#39;BIG&#39;),\n (&#39;d&#39;, &#39;DATA&#39;),\n (&#39;p&#39;, &#39;PROCESSING&#39;),\n (&#39;m&#39;, &#39;MADE&#39;),\n (&#39;s&#39;, &#39;SIMPLE&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["flatMap over the rows \n- to expand the number of rows that you have to make\n- so that each row represents a character\n\nEach character as we converted them into array:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7314f6d-63a1-4b46-b1da-0b8586c54730"}}},{"cell_type":"code","source":["keyword.flatMapValues(lambda word: word.upper()).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8e0535-5146-49e8-bb88-b7d56a4f07d5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[5]: [(&#39;s&#39;, &#39;S&#39;),\n (&#39;s&#39;, &#39;P&#39;),\n (&#39;s&#39;, &#39;A&#39;),\n (&#39;s&#39;, &#39;R&#39;),\n (&#39;s&#39;, &#39;K&#39;),\n (&#39;t&#39;, &#39;T&#39;),\n (&#39;t&#39;, &#39;H&#39;),\n (&#39;t&#39;, &#39;E&#39;),\n (&#39;d&#39;, &#39;D&#39;),\n (&#39;d&#39;, &#39;E&#39;),\n (&#39;d&#39;, &#39;F&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;N&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;T&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;V&#39;),\n (&#39;d&#39;, &#39;E&#39;),\n (&#39;g&#39;, &#39;G&#39;),\n (&#39;g&#39;, &#39;U&#39;),\n (&#39;g&#39;, &#39;I&#39;),\n (&#39;g&#39;, &#39;D&#39;),\n (&#39;g&#39;, &#39;E&#39;),\n (&#39;:&#39;, &#39;:&#39;),\n (&#39;b&#39;, &#39;B&#39;),\n (&#39;b&#39;, &#39;I&#39;),\n (&#39;b&#39;, &#39;G&#39;),\n (&#39;d&#39;, &#39;D&#39;),\n (&#39;d&#39;, &#39;A&#39;),\n (&#39;d&#39;, &#39;T&#39;),\n (&#39;d&#39;, &#39;A&#39;),\n (&#39;p&#39;, &#39;P&#39;),\n (&#39;p&#39;, &#39;R&#39;),\n (&#39;p&#39;, &#39;O&#39;),\n (&#39;p&#39;, &#39;C&#39;),\n (&#39;p&#39;, &#39;E&#39;),\n (&#39;p&#39;, &#39;S&#39;),\n (&#39;p&#39;, &#39;S&#39;),\n (&#39;p&#39;, &#39;I&#39;),\n (&#39;p&#39;, &#39;N&#39;),\n (&#39;p&#39;, &#39;G&#39;),\n (&#39;m&#39;, &#39;M&#39;),\n (&#39;m&#39;, &#39;A&#39;),\n (&#39;m&#39;, &#39;D&#39;),\n (&#39;m&#39;, &#39;E&#39;),\n (&#39;s&#39;, &#39;S&#39;),\n (&#39;s&#39;, &#39;I&#39;),\n (&#39;s&#39;, &#39;M&#39;),\n (&#39;s&#39;, &#39;P&#39;),\n (&#39;s&#39;, &#39;L&#39;),\n (&#39;s&#39;, &#39;E&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [(&#39;s&#39;, &#39;S&#39;),\n (&#39;s&#39;, &#39;P&#39;),\n (&#39;s&#39;, &#39;A&#39;),\n (&#39;s&#39;, &#39;R&#39;),\n (&#39;s&#39;, &#39;K&#39;),\n (&#39;t&#39;, &#39;T&#39;),\n (&#39;t&#39;, &#39;H&#39;),\n (&#39;t&#39;, &#39;E&#39;),\n (&#39;d&#39;, &#39;D&#39;),\n (&#39;d&#39;, &#39;E&#39;),\n (&#39;d&#39;, &#39;F&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;N&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;T&#39;),\n (&#39;d&#39;, &#39;I&#39;),\n (&#39;d&#39;, &#39;V&#39;),\n (&#39;d&#39;, &#39;E&#39;),\n (&#39;g&#39;, &#39;G&#39;),\n (&#39;g&#39;, &#39;U&#39;),\n (&#39;g&#39;, &#39;I&#39;),\n (&#39;g&#39;, &#39;D&#39;),\n (&#39;g&#39;, &#39;E&#39;),\n (&#39;:&#39;, &#39;:&#39;),\n (&#39;b&#39;, &#39;B&#39;),\n (&#39;b&#39;, &#39;I&#39;),\n (&#39;b&#39;, &#39;G&#39;),\n (&#39;d&#39;, &#39;D&#39;),\n (&#39;d&#39;, &#39;A&#39;),\n (&#39;d&#39;, &#39;T&#39;),\n (&#39;d&#39;, &#39;A&#39;),\n (&#39;p&#39;, &#39;P&#39;),\n (&#39;p&#39;, &#39;R&#39;),\n (&#39;p&#39;, &#39;O&#39;),\n (&#39;p&#39;, &#39;C&#39;),\n (&#39;p&#39;, &#39;E&#39;),\n (&#39;p&#39;, &#39;S&#39;),\n (&#39;p&#39;, &#39;S&#39;),\n (&#39;p&#39;, &#39;I&#39;),\n (&#39;p&#39;, &#39;N&#39;),\n (&#39;p&#39;, &#39;G&#39;),\n (&#39;m&#39;, &#39;M&#39;),\n (&#39;m&#39;, &#39;A&#39;),\n (&#39;m&#39;, &#39;D&#39;),\n (&#39;m&#39;, &#39;E&#39;),\n (&#39;s&#39;, &#39;S&#39;),\n (&#39;s&#39;, &#39;I&#39;),\n (&#39;s&#39;, &#39;M&#39;),\n (&#39;s&#39;, &#39;P&#39;),\n (&#39;s&#39;, &#39;L&#39;),\n (&#39;s&#39;, &#39;E&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# in the key–value pair format, we can also extract the specific keys or values:\nkeyword.keys().collect()\nkeyword.values().collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125e84da-1c32-4204-8a08-a92c9a893a0a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Sample an RDD by a set of keys\n- via an approximation or exactly \n- both operations can do so with or without replacement,  or sampling by a fraction by a given key\n- This is done via simple random sampling with one pass over the RDD, which produces a sample of size that’s approximately equal to the sum of math.ceil(numItems *samplingRate) over all key values\n- differs from sampleByKey in that you make additional passes over the RDD toc reate a sample size that’s exactly equal to the sum of math.ceil(numItems * samplingRate) over all key values with a 99.99% confidence\n- When sampling without replacement, you need one additional pass over the RDD to guarantee sample size; when sampling with replacement, you need two additional passes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2c07f78-cd76-4cc7-af31-df98b646c9ec"}}},{"cell_type":"code","source":["import random\ndistinctChars = words.flatMap(lambda word: list(word.lower())).distinct()\\\n  .collect()\nsampleMap = dict(map(lambda c: (c, random.random()), distinctChars))\nwords.map(lambda word: (word.lower()[0], word))\\\n  .sampleByKey(True, sampleMap, 6).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"125b62fc-1de3-4efc-8570-d1f46051d0f3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: [(&#39;t&#39;, &#39;The&#39;),\n (&#39;t&#39;, &#39;The&#39;),\n (&#39;d&#39;, &#39;Definitive&#39;),\n (&#39;d&#39;, &#39;Data&#39;),\n (&#39;s&#39;, &#39;Simple&#39;),\n (&#39;s&#39;, &#39;Simple&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: [(&#39;t&#39;, &#39;The&#39;),\n (&#39;t&#39;, &#39;The&#39;),\n (&#39;d&#39;, &#39;Definitive&#39;),\n (&#39;d&#39;, &#39;Data&#39;),\n (&#39;s&#39;, &#39;Simple&#39;),\n (&#39;s&#39;, &#39;Simple&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Can perform aggregations on plain RDDs or on PairRDDs, depending on the method that you are using: \n\nchars = words.flatMap(lambda word: word.lower())\nKVcharacters = chars.map(lambda letter: (letter, 1))\ndef maxFunc(left, right):\n  return max(left, right)\ndef addFunc(left, right):\n  return left + right\nnums = sc.parallelize(range(1,31), 5)\n\n# with this, you can implement something like countByKey, which counts the items per eachkey\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0bc48fc-c374-416c-a24e-27320b253950"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["KVcharacters.countByKey()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f1c7573c-7a7a-45ff-aa33-fb1ba3000c79"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: defaultdict(int,\n            {&#39;s&#39;: 4,\n             &#39;p&#39;: 3,\n             &#39;a&#39;: 4,\n             &#39;r&#39;: 2,\n             &#39;k&#39;: 1,\n             &#39;t&#39;: 3,\n             &#39;h&#39;: 1,\n             &#39;e&#39;: 7,\n             &#39;d&#39;: 4,\n             &#39;f&#39;: 1,\n             &#39;i&#39;: 7,\n             &#39;n&#39;: 2,\n             &#39;v&#39;: 1,\n             &#39;g&#39;: 3,\n             &#39;u&#39;: 1,\n             &#39;:&#39;: 1,\n             &#39;b&#39;: 1,\n             &#39;o&#39;: 1,\n             &#39;c&#39;: 1,\n             &#39;m&#39;: 2,\n             &#39;l&#39;: 1})</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: defaultdict(int,\n            {&#39;s&#39;: 4,\n             &#39;p&#39;: 3,\n             &#39;a&#39;: 4,\n             &#39;r&#39;: 2,\n             &#39;k&#39;: 1,\n             &#39;t&#39;: 3,\n             &#39;h&#39;: 1,\n             &#39;e&#39;: 7,\n             &#39;d&#39;: 4,\n             &#39;f&#39;: 1,\n             &#39;i&#39;: 7,\n             &#39;n&#39;: 2,\n             &#39;v&#39;: 1,\n             &#39;g&#39;: 3,\n             &#39;u&#39;: 1,\n             &#39;:&#39;: 1,\n             &#39;b&#39;: 1,\n             &#39;o&#39;: 1,\n             &#39;c&#39;: 1,\n             &#39;m&#39;: 2,\n             &#39;l&#39;: 1})</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nimport functools\n\nKVcharacters.groupByKey().map(lambda row: (row[0], reduce(addFunc, row[1])))\\\n  .collect()\n# note this is Python 2, reduce must be imported from functools in Python 3\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e74503f-b213-4834-93e2-dfd53c6e53ed"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1762652494941869&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">import</span> functools\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>KVcharacters<span class=\"ansi-blue-fg\">.</span>groupByKey<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> reduce<span class=\"ansi-blue-fg\">(</span>addFunc<span class=\"ansi-blue-fg\">,</span> row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\"># note this is Python 2, reduce must be imported from functools in Python 3</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 43, ip-10-172-244-18.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;reduce&#39; is not defined&#39;, from &lt;command-1762652494941869&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1762652494941869&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;reduce&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;reduce&#39; is not defined&#39;, from &lt;command-1762652494941869&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1762652494941869&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;reduce&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 43, ip-10-172-244-18.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;reduce&#39; is not defined&#39;, from &lt;command-1762652494941869&gt;, line 3. Full traceback below:","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1762652494941869&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">import</span> functools\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>KVcharacters<span class=\"ansi-blue-fg\">.</span>groupByKey<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> <span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> reduce<span class=\"ansi-blue-fg\">(</span>addFunc<span class=\"ansi-blue-fg\">,</span> row<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> <span class=\"ansi-red-fg\"># note this is Python 2, reduce must be imported from functools in Python 3</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    901</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    902</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 903</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    904</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    905</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 21.0 failed 1 times, most recent failure: Lost task 1.0 in stage 21.0 (TID 43, ip-10-172-244-18.us-west-2.compute.internal, executor driver): org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;reduce&#39; is not defined&#39;, from &lt;command-1762652494941869&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1762652494941869&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;reduce&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:260)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: &#39;NameError: name &#39;reduce&#39; is not defined&#39;, from &lt;command-1762652494941869&gt;, line 3. Full traceback below:\nTraceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 654, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 646, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 279, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File &#34;/databricks/spark/python/pyspark/util.py&#34;, line 109, in wrapper\n    return f(*args, **kwargs)\n  File &#34;&lt;command-1762652494941869&gt;&#34;, line 3, in &lt;lambda&gt;\nNameError: name &#39;reduce&#39; is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:598)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:733)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:716)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:551)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1011)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2371)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:662)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:665)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["reduceByKey\n- more stable approach is to perform a flatMap\n- then perform a map, to map each letter instance to the number one, and then perform a reduceByKey with a summation function in order to collect back the array\n- implementation is much more stable because the reduce happens within each partition and doesn’t need to put everything in memory\n- no incurred shuffle during this operation; everything happens at each worker individually before performing a final reduce.\n- greatly enhances the speed at which you can perform the operation as well as the stability of the operation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3db42cb6-fad5-4cda-9932-728aa9ba1464"}}},{"cell_type":"code","source":["KVcharacters.reduceByKey(addFunc).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb0a4746-d915-4ff0-a756-bd1f402a4799"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[20]: [(&#39;s&#39;, 4),\n (&#39;p&#39;, 3),\n (&#39;r&#39;, 2),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 4),\n (&#39;i&#39;, 7),\n (&#39;g&#39;, 3),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 4),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 3),\n (&#39;e&#39;, 7),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 2),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: [(&#39;s&#39;, 4),\n (&#39;p&#39;, 3),\n (&#39;r&#39;, 2),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 4),\n (&#39;i&#39;, 7),\n (&#39;g&#39;, 3),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 4),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 3),\n (&#39;e&#39;, 7),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 2),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["aggregate\n-  function requires a null and start value and then requires you to specify two different functions\n- first aggregates within partitions, the second aggregates across partitions\n- start value will be used at both aggregation levels\n- does have some performance implications because it performs the final aggregation on the driver \n- If the results from the executors are too large, they can take down the driver with an OutOfMemoryError"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2046cfde-0eb0-4b2f-aad1-cc4362f7cde7"}}},{"cell_type":"code","source":["nums.aggregate(0, maxFunc, addFunc)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"243c4fa1-3100-4584-95b9-c526e65c316b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[21]: 90</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: 90</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["treeAggregate \n- does the same thing as aggregate (at the user level) but does so in a different way\n- basically “pushes down” some of the subaggregations (creating a tree from executor to executor) before performing the final aggregation on the driver\n- multiple levels can help you to ensure that the driver does not run out of memory in the process of the aggregation\n- these tree-based implementations are often to try to improve stability in certain operations"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a521cf5-0ba0-4718-ba89-0298da43ba04"}}},{"cell_type":"code","source":["depth = 3\nnums.treeAggregate(0, maxFunc, addFunc, depth)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a42afffa-2bc2-4793-bf32-e47a41775db7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[22]: 90</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[22]: 90</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# aggregateByKey\n# - This function does the same as aggregate but instead of doing it partition by partition, it does it by key\n\nKVcharacters.aggregateByKey(0, addFunc, maxFunc).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f67eb97d-a403-43e2-b998-c998e6f5d4cf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[23]: [(&#39;s&#39;, 3),\n (&#39;p&#39;, 2),\n (&#39;r&#39;, 1),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 2),\n (&#39;i&#39;, 4),\n (&#39;g&#39;, 2),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 3),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 2),\n (&#39;e&#39;, 4),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 1),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[23]: [(&#39;s&#39;, 3),\n (&#39;p&#39;, 2),\n (&#39;r&#39;, 1),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 2),\n (&#39;i&#39;, 4),\n (&#39;g&#39;, 2),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 3),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 2),\n (&#39;e&#39;, 4),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 1),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["combineByKey\n- Instead of specifying an aggregation function, you can specify a combiner\n- combiner operates on a given key and merges the values according to some function\n- then goes to merge the different outputs of the combiners to give us our result\n- can specify the number of output partitions as a custom output partitioner as well"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e34c877-1c61-49e5-98f7-9008e94e7bd1"}}},{"cell_type":"code","source":["def valToCombiner(value):\n  return [value]\ndef mergeValuesFunc(vals, valToAppend):\n  vals.append(valToAppend)\n  return vals\ndef mergeCombinerFunc(vals1, vals2):\n  return vals1 + vals2\noutputPartitions = 6\nKVcharacters\\\n  .combineByKey(\n    valToCombiner,\n    mergeValuesFunc,\n    mergeCombinerFunc,\n    outputPartitions)\\\n  .collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abb98614-f253-4934-9c1a-4257a8de9b0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[24]: [(&#39;s&#39;, [1, 1, 1, 1]),\n (&#39;d&#39;, [1, 1, 1, 1]),\n (&#39;l&#39;, [1]),\n (&#39;v&#39;, [1]),\n (&#39;:&#39;, [1]),\n (&#39;p&#39;, [1, 1, 1]),\n (&#39;r&#39;, [1, 1]),\n (&#39;c&#39;, [1]),\n (&#39;k&#39;, [1]),\n (&#39;t&#39;, [1, 1, 1]),\n (&#39;n&#39;, [1, 1]),\n (&#39;u&#39;, [1]),\n (&#39;o&#39;, [1]),\n (&#39;h&#39;, [1]),\n (&#39;i&#39;, [1, 1, 1, 1, 1, 1, 1]),\n (&#39;g&#39;, [1, 1, 1]),\n (&#39;b&#39;, [1]),\n (&#39;a&#39;, [1, 1, 1, 1]),\n (&#39;e&#39;, [1, 1, 1, 1, 1, 1, 1]),\n (&#39;f&#39;, [1]),\n (&#39;m&#39;, [1, 1])]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[24]: [(&#39;s&#39;, [1, 1, 1, 1]),\n (&#39;d&#39;, [1, 1, 1, 1]),\n (&#39;l&#39;, [1]),\n (&#39;v&#39;, [1]),\n (&#39;:&#39;, [1]),\n (&#39;p&#39;, [1, 1, 1]),\n (&#39;r&#39;, [1, 1]),\n (&#39;c&#39;, [1]),\n (&#39;k&#39;, [1]),\n (&#39;t&#39;, [1, 1, 1]),\n (&#39;n&#39;, [1, 1]),\n (&#39;u&#39;, [1]),\n (&#39;o&#39;, [1]),\n (&#39;h&#39;, [1]),\n (&#39;i&#39;, [1, 1, 1, 1, 1, 1, 1]),\n (&#39;g&#39;, [1, 1, 1]),\n (&#39;b&#39;, [1]),\n (&#39;a&#39;, [1, 1, 1, 1]),\n (&#39;e&#39;, [1, 1, 1, 1, 1, 1, 1]),\n (&#39;f&#39;, [1]),\n (&#39;m&#39;, [1, 1])]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["foldByKey\n-  merges the values for each key using an associative function and a neutral “zero value\"\n- can be added to the result an arbitrary number of times\n-  must not change the result (e.g., 0 for addition, or 1 for multiplication)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fe44ff1-4c13-4200-baf5-c21c9ae37c23"}}},{"cell_type":"code","source":["\nKVcharacters.foldByKey(0, addFunc).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35b7b10e-8637-407b-844f-b82371756369"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[25]: [(&#39;s&#39;, 4),\n (&#39;p&#39;, 3),\n (&#39;r&#39;, 2),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 4),\n (&#39;i&#39;, 7),\n (&#39;g&#39;, 3),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 4),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 3),\n (&#39;e&#39;, 7),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 2),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: [(&#39;s&#39;, 4),\n (&#39;p&#39;, 3),\n (&#39;r&#39;, 2),\n (&#39;h&#39;, 1),\n (&#39;d&#39;, 4),\n (&#39;i&#39;, 7),\n (&#39;g&#39;, 3),\n (&#39;b&#39;, 1),\n (&#39;c&#39;, 1),\n (&#39;l&#39;, 1),\n (&#39;a&#39;, 4),\n (&#39;k&#39;, 1),\n (&#39;t&#39;, 3),\n (&#39;e&#39;, 7),\n (&#39;f&#39;, 1),\n (&#39;n&#39;, 2),\n (&#39;v&#39;, 1),\n (&#39;u&#39;, 1),\n (&#39;:&#39;, 1),\n (&#39;o&#39;, 1),\n (&#39;m&#39;, 2)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["CoGroups\n-  give you the ability to group together up to three key–value RDDs together in Scala and two in Python\n- joins the given values by key\n- effectively just a group-based joinon an RDD\n-  can also specify a number of output partitions or a custom partitioning function to control exactly how this data is distributed across the cluster"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"65ce3681-1863-4ab0-ae1c-74025bd72183"}}},{"cell_type":"code","source":["import random\ndistinctChars = words.flatMap(lambda word: word.lower()).distinct()\ncharRDD = distinctChars.map(lambda c: (c, random.random()))\ncharRDD2 = distinctChars.map(lambda c: (c, random.random()))\ncharRDD.cogroup(charRDD2).take(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8abe7a57-0e24-4428-aa4f-b9f17add2f81"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[26]: [(&#39;s&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171626f90&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb171626490&gt;)),\n (&#39;p&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171626c90&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb171626590&gt;)),\n (&#39;r&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb17168b910&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb17168b610&gt;)),\n (&#39;i&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb1715da310&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb1786cb650&gt;)),\n (&#39;g&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171630f10&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb1716302d0&gt;))]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[26]: [(&#39;s&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171626f90&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb171626490&gt;)),\n (&#39;p&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171626c90&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb171626590&gt;)),\n (&#39;r&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb17168b910&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb17168b610&gt;)),\n (&#39;i&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb1715da310&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb1786cb650&gt;)),\n (&#39;g&#39;,\n  (&lt;pyspark.resultiterable.ResultIterable at 0x7fb171630f10&gt;,\n   &lt;pyspark.resultiterable.ResultIterable at 0x7fb1716302d0&gt;))]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["joins\n- RDDs have much the same joins as we saw in the Structured API\n- although RDDs are much more involved for you\n- follow the same basic format: \n\t- the two RDDs we would like to join\n\t- optionally, either - \n\t\t-  the number of output partitions \n\t\t- or the customer partition function to which they should output"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e290310-0358-4b4e-ab5c-757066326d2f"}}},{"cell_type":"code","source":["keyedChars = distinctChars.map(lambda c: (c, random.random()))\noutputPartitions = 10\nKVcharacters.join(keyedChars).count()\nKVcharacters.join(keyedChars, outputPartitions).count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bdf3912-6406-4ac7-97ea-e3af52276913"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Zips\n- combine two RDDs, so it’s worth labeling it as a join\n- zip allows you to “zip” together two RDDs, assuming that they have the same length\n- creates a PairRDD\n- The two RDDs must have the same number of partitions as well as the same number of elements"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba8bb6a8-5d1a-4d6a-adf4-cd5a6457990c"}}},{"cell_type":"code","source":["numRange = sc.parallelize(range(10), 2)\nwords.zip(numRange).collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b37c5ff-9fb2-499a-954e-088c1e4c2ac2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[27]: [(&#39;Spark&#39;, 0),\n (&#39;The&#39;, 1),\n (&#39;Definitive&#39;, 2),\n (&#39;Guide&#39;, 3),\n (&#39;:&#39;, 4),\n (&#39;Big&#39;, 5),\n (&#39;Data&#39;, 6),\n (&#39;Processing&#39;, 7),\n (&#39;Made&#39;, 8),\n (&#39;Simple&#39;, 9)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[27]: [(&#39;Spark&#39;, 0),\n (&#39;The&#39;, 1),\n (&#39;Definitive&#39;, 2),\n (&#39;Guide&#39;, 3),\n (&#39;:&#39;, 4),\n (&#39;Big&#39;, 5),\n (&#39;Data&#39;, 6),\n (&#39;Processing&#39;, 7),\n (&#39;Made&#39;, 8),\n (&#39;Simple&#39;, 9)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["coalesce\n-  effectively collapses partitions on the same worker in order to avoid a shuffle of the data when repartitioning\nex.  if our words RDD is currently two partitions, we can collapse that to one partition by using coalesce without bringing about a shuffle of the data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fa2447b-e008-42c5-b083-9eee25408c59"}}},{"cell_type":"code","source":["words.coalesce(1).getNumPartitions() # 1\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4feeca29-0454-4aca-bbd9-1b8ef6fc9276"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[29]: 1</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: 1</div>"]}}],"execution_count":0},{"cell_type":"code","source":["words.repartition(10)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3be50b2c-f17c-459d-9532-17c3acfabca9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[32]: MapPartitionsRDD[111] at coalesce at NativeMethodAccessorImpl.java:0</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[32]: MapPartitionsRDD[111] at coalesce at NativeMethodAccessorImpl.java:0</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Custom Partitioning\n- one of the primary reasons you’d want to use RDDs\n- Custom partitioners are not available in the Structured APIs because they don’t really have a logical counterpart\n- They’re a low-level, implementation detail that can have a significant effect on whether your jobs run successfully\n- sole goal of custom partitioning is to even out the distribution of your data across the cluster so that you can work around problems like data skew\n- If you’re going to use custom partitioners, you should drop down to RDDs from the StructuredAPIs, apply your custom partitioner, and then convert it back to a DataFrame or Dataset\n- get the best of both worlds, only dropping down to custom partitioning when you need to\n- To perform custom partitioning you need to implement your own class that extends Partitioner. \n- only when you have lots of domain knowledge about your problem space \n- if you’re just looking to partition on a value or even a set of values (columns), it’s worth just doing it in the DataFrame API"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"571ee4cf-f7b9-4aab-8812-e8ba162b9ffd"}}},{"cell_type":"code","source":["df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\")\\\n  .csv(\"/FileStore/tables/online_retail_dataset.csv\")\nrdd = df.coalesce(10).rdd\n\ndf.printSchema()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2cc66307-0b8c-4c08-875e-ef955a8af5c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":true,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":true,"type":"integer"},{"metadata":{},"name":"Country","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: integer (nullable = true)\n |-- Country: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- InvoiceNo: string (nullable = true)\n-- StockCode: string (nullable = true)\n-- Description: string (nullable = true)\n-- Quantity: integer (nullable = true)\n-- InvoiceDate: string (nullable = true)\n-- UnitPrice: double (nullable = true)\n-- CustomerID: integer (nullable = true)\n-- Country: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Spark has two built-in Partitioners that you can leverage off in the RDD API\n- HashPartitioner for discrete values \n- RangePartitioner - continuous values\n- Spark’s Structured APIs already use these - but we can use the same thing in RDDs\n- hash and range partitioners are useful but rudimentary\n- At times, you will need to perform some very low-level partitioning because you’re working with very large data and large key skew \n- Key skew simply means that some keys have many, many more values than other keys\n- You want to break these keys as much as possible to improve parallelism and prevent OutOfMemoryErrors during the course of execution\n- Might be that you need to partition more keys if and only if the key matches a certain format. \n\nEx. \n- might know that there are two customers in your dataset that always crash your analysis and we need to break them up further than other customer IDs\n- these two are so skewed that they need to be operated on alone \n- whereas all of the others can be lumped into large groups"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9573a45-ffc5-4c6e-b2a3-a36849790d0c"}}},{"cell_type":"code","source":["def partitionFunc(key):\n  import random\n  if key == 17850 or key == 12583:\n    return 0\n  else:\n    return random.randint(1,2)\n\nkeyedRDD = rdd.keyBy(lambda row: row[6])\nkeyedRDD\\\n  .partitionBy(3, partitionFunc)\\\n  .map(lambda x: x[0])\\\n  .glom()\\\n  .map(lambda x: len(set(x)))\\\n  .take(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3771dc5-eda9-440a-a81e-25823a36399a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[34]: [2, 4298, 4311]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[34]: [2, 4298, 4311]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7264971c-bbfb-4646-98f5-1646e869e007"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Low_Level_APIs-Chapter_13_Advanced_RDDs","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1762652494941859}},"nbformat":4,"nbformat_minor":0}
