{"cells":[{"cell_type":"markdown","source":["One column containing 1,000rows with values from 0 to 999. This range of numbers represents a distributed collection. When run on a cluster, each part of this range of numbers exists on a different executor."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dc5e8fd-10ab-4b98-8ae0-b90dde61a41f"}}},{"cell_type":"code","source":["myRange = spark.range(1000).toDF(\"number\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f13b270-4b62-429c-949a-e3ba47f569bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"myRange","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Returns no output - specified only an abstract transformation - Spark will not act on transformations until we call an action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51a55996-6d5b-4b2b-96e5-58a458acf2cf"}}},{"cell_type":"code","source":["divisBy2 = myRange.where(\"number % 2 = 0\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f21fc76b-5e55-4bf1-b434-31c0aa0e56a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"divisBy2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["To trigger the computation,we run an action. An action instructs Spark to compute a result from a series of transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eec78a4c-1049-44fb-a905-3884835f27e1"}}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"298c2284-7a3b-4fe7-a067-1cdd1e07c9b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: 500</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: 500</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Schema inference, which means that we want Spark to take a best guess at whatthe schema of our DataFrame should be. Number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28f0123a-c29c-4cd1-b23c-5af0d0bab48e"}}},{"cell_type":"code","source":["flightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/FileStore/tables/2015_summary.csv\")\n\n\n# \"/FileStore/tables/2015_summary.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba364c31-fc0f-4906-a461-27e5f6dc7728"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"flightData2015","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"ORIGIN_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Stores table for a particular Spark session."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e327285b-97d5-4da0-84fe-d4f038f21416"}}},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97a44955-a670-4584-ba3f-9b74089de529"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. Explain can be called on any DataFrame object to see the DataFrame’s lineage (how Spark will execute query). The sort of our data is actually a wide transformation because rows will need to be compared with one another."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42d5208b-3a45-466c-a055-238c01be593e"}}},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"822f73bc-8731-4697-9840-354b8d34b09d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["By default, when we perform a shuffle, Sparkoutputs 200 shuffle partitions. Let’s set this value to 5 to reduce the number of the output partitions from the shuffle:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8d02bf6-9cde-4a25-a19b-ca795f84a996"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\nflightData2015.sort(\"count\").take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da77df78-8449-4364-b6ba-122525b13bac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\nsqlWay.explain()\ndataFrameWay.explain()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59fe05de-d0f9-454c-b3d9-4b90934d7854"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"sqlWay","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"dataFrameWay","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f4f133-5bf6-4930-a111-c3aff547be35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [Row(max(count)=370002)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [Row(max(count)=370002)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f32c3f3f-4605-4c5f-be5c-a805d1644eb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"maxSql","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"destination_total","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["DataFrame syntax that is semantically similar but slightly different inimplementation and ordering:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4422e797-7e1e-483b-9f2d-e27989f48e8d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67e51ab7-7496-4b88-a246-bf1d8afc1a22"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Aggregation happens in two phases, in the partial_sum calls. This is because summing a list of numbers is commutative, and Spark can perform the sum, partition by partition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1238726b-a77f-40ab-855f-0a26ae3c7ae4"}}},{"cell_type":"code","source":["flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8e8fc1f-035b-4dbb-bd6f-0db9aabce76a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c738ee2-b194-4770-ac64-32ae67217a15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1163461417268357}},"nbformat":4,"nbformat_minor":0}
