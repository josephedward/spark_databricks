{"cells":[{"cell_type":"markdown","source":["One column containing 1,000rows with values from 0 to 999. This range of numbers represents a distributed collection. When run on a cluster, each part of this range of numbers exists on a different executor."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3dc5e8fd-10ab-4b98-8ae0-b90dde61a41f"}}},{"cell_type":"code","source":["myRange = spark.range(1000).toDF(\"number\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f13b270-4b62-429c-949a-e3ba47f569bd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"myRange","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Returns no output - specified only an abstract transformation - Spark will not act on transformations until we call an action."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"51a55996-6d5b-4b2b-96e5-58a458acf2cf"}}},{"cell_type":"code","source":["divisBy2 = myRange.where(\"number % 2 = 0\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f21fc76b-5e55-4bf1-b434-31c0aa0e56a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"divisBy2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["To trigger the computation,we run an action. An action instructs Spark to compute a result from a series of transformations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eec78a4c-1049-44fb-a905-3884835f27e1"}}},{"cell_type":"code","source":["divisBy2.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"298c2284-7a3b-4fe7-a067-1cdd1e07c9b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: 500</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: 500</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Schema inference, which means that we want Spark to take a best guess at whatthe schema of our DataFrame should be. Number of rows is unspecified is because reading data is a transformation, and is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types each column should be."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28f0123a-c29c-4cd1-b23c-5af0d0bab48e"}}},{"cell_type":"code","source":["flightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(\"/FileStore/tables/2015_summary.csv\")\n\n\n# \"/FileStore/tables/2015_summary.csv\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba364c31-fc0f-4906-a461-27e5f6dc7728"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"flightData2015","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"ORIGIN_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Stores table for a particular Spark session."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e327285b-97d5-4da0-84fe-d4f038f21416"}}},{"cell_type":"code","source":["flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97a44955-a670-4584-ba3f-9b74089de529"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can see that Spark is building up a plan for how it will execute this across the cluster by looking at the explain plan. Explain can be called on any DataFrame object to see the DataFrameâ€™s lineage (how Spark will execute query). The sort of our data is actually a wide transformation because rows will need to be compared with one another."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42d5208b-3a45-466c-a055-238c01be593e"}}},{"cell_type":"code","source":["flightData2015.sort(\"count\").explain()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"822f73bc-8731-4697-9840-354b8d34b09d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   Sort [count#65 ASC NULLS FIRST], true, 0\n   +- Exchange rangepartitioning(count#65 ASC NULLS FIRST, 200), true, [id=#632]\n      +- FileScan csv [DEST_COUNTRY_NAME#63,ORIGIN_COUNTRY_NAME#64,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["By default, when we perform a shuffle, Sparkoutputs 200 shuffle partitions. Letâ€™s set this value to 5 to reduce the number of the output partitions from the shuffle:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8d02bf6-9cde-4a25-a19b-ca795f84a996"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")\nflightData2015.sort(\"count\").take(2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da77df78-8449-4364-b6ba-122525b13bac"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[14]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[14]: [Row(DEST_COUNTRY_NAME=&#39;United States&#39;, ORIGIN_COUNTRY_NAME=&#39;Singapore&#39;, count=1),\n Row(DEST_COUNTRY_NAME=&#39;Moldova&#39;, ORIGIN_COUNTRY_NAME=&#39;United States&#39;, count=1)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\nsqlWay.explain()\ndataFrameWay.explain()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"59fe05de-d0f9-454c-b3d9-4b90934d7854"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"sqlWay","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count(1)","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null},{"name":"dataFrameWay","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#232L) AS count(1)#220L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#453]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#232L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n+- == Initial Plan ==\n   HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_count(merge count#237L) AS count(1)#227L])\n   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#499]\n      +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_count(1) AS count#237L])\n         +- FileScan csv [DEST_COUNTRY_NAME#63] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import max\n\nflightData2015.select(max(\"count\")).take(1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f4f133-5bf6-4930-a111-c3aff547be35"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[6]: [Row(max(count)=370002)]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[6]: [Row(max(count)=370002)]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["maxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f32c3f3f-4605-4c5f-be5c-a805d1644eb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"maxSql","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"DEST_COUNTRY_NAME","nullable":true,"type":"string"},{"metadata":{},"name":"destination_total","nullable":true,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\">+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["DataFrame syntax that is semantically similar but slightly different inimplementation and ordering:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4422e797-7e1e-483b-9f2d-e27989f48e8d"}}},{"cell_type":"code","source":["from pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67e51ab7-7496-4b88-a246-bf1d8afc1a22"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------------+-----------------+\n|DEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n|    United States|           411352|\n|           Canada|             8399|\n|           Mexico|             7140|\n|   United Kingdom|             2025|\n|            Japan|             1548|\n+-----------------+-----------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Aggregation happens in two phases, in the partial_sum calls. This is because summing a list of numbers is commutative, and Spark can perform the sum, partition by partition."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1238726b-a77f-40ab-855f-0a26ae3c7ae4"}}},{"cell_type":"code","source":["flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .explain()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8e8fc1f-035b-4dbb-bd6f-0db9aabce76a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Current Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n+- == Initial Plan ==\n   TakeOrderedAndProject(limit=5, orderBy=[destination_total#167L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#63,destination_total#167L])\n   +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[finalmerge_sum(merge sum#171L) AS sum(cast(count#65 as bigint))#163L])\n      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#63, 200), true, [id=#387]\n         +- HashAggregate(keys=[DEST_COUNTRY_NAME#63], functions=[partial_sum(cast(count#65 as bigint)) AS sum#171L])\n            +- FileScan csv [DEST_COUNTRY_NAME#63,count#65] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[dbfs:/FileStore/tables/2015_summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,count:int&gt;\n\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c738ee2-b194-4770-ac64-32ae67217a15"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"A_Gentle_Introduction_to_Spark-Chapter_2_A_Gentle_Introduction_to_Spark","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1163461417268357}},"nbformat":4,"nbformat_minor":0}
