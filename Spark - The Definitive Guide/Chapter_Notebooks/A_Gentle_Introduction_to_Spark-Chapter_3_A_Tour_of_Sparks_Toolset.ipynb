{"cells":[{"cell_type":"markdown","source":["Structured Streaming - take the same operations that you perform in batch mode using Spark’s structured APIs,and run them in a streaming fashion"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"591dc4a0-a1ae-442f-ab37-929c17171657"}}},{"cell_type":"code","source":["staticDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"/FileStore/tables/retail-data-by-day/*.csv\")\n\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\nstaticSchema = staticDataFrame.schema\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad8e89f3-42a6-434e-9f75-c3126b57f2b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"staticDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":true,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":true,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Sale hours during which a given customer (identified by CustomerId) makes a large purchase. Add a total cost column and see on what days a customer spent the most"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6089a8ad-5187-4b6c-a558-c0524216bad6"}}},{"cell_type":"code","source":["from pyspark.sql.functions import window, column, desc, col\nstaticDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\\\n  .sort(desc(\"sum(total_cost)\"))\\\n  .show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cfb96f1-6ffa-4e44-8bb8-6dd7ea0efc1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+--------------------+------------------+\n|CustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n|   17450.0|[2011-09-20 00:00...|          71601.44|\n|      null|[2011-11-14 00:00...|          55316.08|\n|      null|[2011-11-07 00:00...|          42939.17|\n|      null|[2011-12-08 00:00...|31975.590000000007|\n|   18102.0|[2011-09-15 00:00...|31661.540000000005|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   17450.0|[2011-09-20 00:00...|          71601.44|\n      null|[2011-11-14 00:00...|          55316.08|\n      null|[2011-11-07 00:00...|          42939.17|\n      null|[2011-12-08 00:00...|31975.590000000007|\n   18102.0|[2011-09-15 00:00...|31661.540000000005|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["good practice to set the number of shuffle partitions to a better fit to reduce costs. This configuration specifies the number of partitions that should be created after a shuffle. By default, the value is 200, but for illustration we can pretend there aren’t many executors on this machine, so \nit’s worth reducing this to 5."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c046fe27-af9e-4836-bb04-921f66774139"}}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\",\"5\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c367b8fb-3738-432e-8ef3-b40c73f603b3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Very little actually changes about the code when Streaming - biggest change using readStream instead of read, and maxFilesPerTrigger option, which specifies  number of files we read in at once. This is to demonstrate “streaming,” and in a production scenario would probably be omitted."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"84c0a558-b066-4e72-b9c1-e283e27ff873"}}},{"cell_type":"code","source":["streamingDataFrame = spark.readStream\\\n    .schema(staticSchema)\\\n    .option(\"maxFilesPerTrigger\", 1)\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .load(\"/FileStore/tables/retail-data-by-day/*.csv\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c25b1a20-6ec9-48fc-9ae9-ff432238893f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"streamingDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":true,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":true,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Can see whether our DataFrame is streaming:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76ee6118-7212-48a5-bd1c-0267a533d78f"}}},{"cell_type":"code","source":["streamingDataFrame.isStreaming "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cc75449-0c5d-4974-a45d-a66943c8e9a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[131]: True</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[131]: True</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Same business logic as the manipulation, perform summation in the process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d570c1e-5379-4a05-a72a-aec1c82028c8"}}},{"cell_type":"code","source":["purchaseByCustomerPerHour = streamingDataFrame\\\n  .selectExpr(\n    \"CustomerId\",\n    \"(UnitPrice * Quantity) as total_cost\",\n    \"InvoiceDate\")\\\n  .groupBy(\n    col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n  .sum(\"total_cost\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1bed96f-2fdf-47a6-9df7-448a436a86af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"purchaseByCustomerPerHour","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"CustomerId","nullable":true,"type":"double"},{"metadata":{},"name":"window","nullable":false,"type":{"fields":[{"metadata":{},"name":"start","nullable":true,"type":"timestamp"},{"metadata":{},"name":"end","nullable":true,"type":"timestamp"}],"type":"struct"}},{"metadata":{},"name":"sum(total_cost)","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Still a lazy operation, so we will need to call a streaming action to start the execution of data flow. Streaming actions are bit different from conventional static action because they populate data instead of just calling something like count (which doesn’t make sense on a stream anyways). The action we will use will output to an in-memory table that will update after each trigger. Each trigger is based on an individual file (the readoption that we set). Spark will mutate the data in the in-memory table such that we will always have the highest value (as specified in our previous aggregation)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6bcd22a2-1e3a-4ca1-88c2-9a5a2aa22a5a"}}},{"cell_type":"code","source":["\n\n# %sh\n# du --human-readable --max-depth=1 --exclude='/dbfs'/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e0524da-6540-44c2-bbe4-4f54d44c754f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sh\n# rm -rf /dbfs/FileStore/plots/*.png"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ec04836-4736-4408-8fda-4223ba5b5e11"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# %sh\n# rm -rf /dbfs/tmp/*\n# rm -rf /dbfs/local_disk0/tmp/*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ad47081-e4bb-480f-af9e-1270db62bb49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Can run queries against the Stream to debug the result if we were to write this out to a production sink:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e7bef124-cb23-4e75-a4ac-7bb25480f7a0"}}},{"cell_type":"code","source":["spark.sql(\"\"\"\n  SELECT *\n  FROM customer_purchases\n  ORDER BY `sum(total_cost)` DESC\n  \"\"\")\\\n  .show(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7206009-fcfb-4c23-8bac-48b9864961b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+--------------------+------------------+\n|CustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n|   12415.0|[2011-06-15 00:00...| 23426.81000000001|\n|   17949.0|[2011-06-30 00:00...|18854.780000000002|\n|      null|[2011-05-10 00:00...| 17949.28000000001|\n|   18102.0|[2011-06-09 00:00...|           16488.0|\n|   14646.0|[2011-05-12 00:00...|16478.460000000006|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   12415.0|[2011-06-15 00:00...| 23426.81000000001|\n   17949.0|[2011-06-30 00:00...|18854.780000000002|\n      null|[2011-05-10 00:00...| 17949.28000000001|\n   18102.0|[2011-06-09 00:00...|           16488.0|\n   14646.0|[2011-05-12 00:00...|16478.460000000006|\n+----------+--------------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Option to write the results out to the console:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d580f99c-146f-4d44-b58f-ecfacec025f3"}}},{"cell_type":"code","source":["# purchaseByCustomerPerHour.writeStream\n# .format(\"console\")\n# .queryName(\"customer_purchases_2\")\n# .outputMode(\"complete\")\n# .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"575890f3-3787-4b3a-ad98-30ab5eda37ce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Shouldn’t use either of these streaming methods in production \n  - convenient demonstration of Structured Streaming’s power. \n  - Window is built on event time, not the time at which Spark processes the data\n  - Shortcoming of Spark Streaming that Structured Streaming has resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0351cd8e-18e3-4c77-9fd5-22471130eec8"}}},{"cell_type":"code","source":["staticDataFrame.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29d07c59-28eb-4f16-a832-ee773b4dcbff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: string (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- InvoiceNo: string (nullable = true)\n-- StockCode: string (nullable = true)\n-- Description: string (nullable = true)\n-- Quantity: integer (nullable = true)\n-- InvoiceDate: string (nullable = true)\n-- UnitPrice: double (nullable = true)\n-- CustomerID: double (nullable = true)\n-- Country: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Begin with some raw data, build up transformations before getting the data into the right format, at which point we can actually train our model and then serve predictions. Transform this data into some numerical representation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"606cb6e0-a334-4aa3-b991-5a1a0c32a94b"}}},{"cell_type":"code","source":["from pyspark.sql.functions import date_format, col\npreppedDataFrame = staticDataFrame\\\n  .na.fill(0)\\\n  .withColumn(\"day_of_week\", date_format(col(\"InvoiceDate\"), \"EEEE\"))\\\n  .coalesce(5)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d785d0ef-d0af-446b-bdcc-df27efe88186"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"preppedDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":false,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":false,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"day_of_week","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["split the data into training and test sets"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ee890660-ccfc-4fbf-bb63-981b47e3074f"}}},{"cell_type":"code","source":["trainDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate < '2011-07-01'\")\ntestDataFrame = preppedDataFrame\\\n  .where(\"InvoiceDate >= '2011-07-01'\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67604775-5ad3-40bb-b19e-e0dd3071fdc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"trainDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":false,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":false,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"day_of_week","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null},{"name":"testDataFrame","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":false,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":false,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"day_of_week","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["splits our dataset roughly in half"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1549fb2e-f396-4a4a-be24-71b17d71f192"}}},{"cell_type":"code","source":["trainDataFrame.count()\ntestDataFrame.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cea7a042-0c7d-44a2-b3c6-8ca9570ed28a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[141]: 296006</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[141]: 296006</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**StringIndexer** - turns our days of weeks into corresponding numerical values. Represents Saturday as 6, and Monday as 1. Numbering scheme implicitly states that Saturday is greater than Monday (by pure numerical values)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec775e31-b88f-4bb0-a619-2f906179f7cb"}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\nindexer = StringIndexer()\\\n  .setInputCol(\"day_of_week\")\\\n  .setOutputCol(\"day_of_week_index\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15cae137-2dc1-4797-97e7-6ea29839dd75"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We want Monday to be greater than Saturday. Use a OneHotEncoder to encode each of these values as their own column. These Boolean flags state whether that day of week is the relevant day of the week."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"787aac49-5cd3-4072-a7c4-0120beaed711"}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nencoder = OneHotEncoder()\\\n  .setInputCol(\"day_of_week_index\")\\\n  .setOutputCol(\"day_of_week_encoded\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8df6a69f-62c4-4c5b-8cb7-1214a3126b80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Each of these will result in a set of columns that we will “assemble” into a vector. All machine learning algorithms in Spark take as input a Vector type, which must be a set of numerical values. Three key features: the price, the quantity, and the day of week."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a01caca7-7c97-4f15-accd-3fc124867f61"}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\nvectorAssembler = VectorAssembler()\\\n  .setInputCols([\"UnitPrice\", \"Quantity\", \"day_of_week_encoded\"])\\\n  .setOutputCol(\"features\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b1c8260b-7864-43f6-874e-a5caff23f011"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Set up a pipeline so that future data we need to transform can go through the exact same process:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0939fd40-e086-4526-9e0f-a01e2dff04c7"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\ntransformationPipeline = Pipeline()\\\n  .setStages([indexer, encoder, vectorAssembler])\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7d96342f-21bc-422c-88c4-babcbf167c71"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Fit our transformers to this dataset. StringIndexer needs to know how many unique values there are to be indexed. Spark must look at all the distinct values in the column to be indexed in order to store those values later on."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"945df275-f9db-4131-989e-9ca44c125a1c"}}},{"cell_type":"code","source":["fittedPipeline = transformationPipeline.fit(trainDataFrame)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7de62ceb-43a4-4947-94cb-5d68910ccfad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Take that fitted pipeline and use it to transform our data in a consistent and repeatable way:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fcab461-9acd-40e2-8486-347e81f6979e"}}},{"cell_type":"code","source":["transformedTraining = fittedPipeline.transform(trainDataFrame)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"abac0434-9126-4084-a3ea-e70881432cce"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"transformedTraining","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":false,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":false,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"day_of_week","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"day_of_week_index","type":"nominal","vals":["Thursday","Tuesday","Wednesday","Monday","Friday","Sunday"]}},"name":"day_of_week_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"Thursday"},{"idx":1,"name":"Tuesday"},{"idx":2,"name":"Wednesday"},{"idx":3,"name":"Monday"},{"idx":4,"name":"Friday"}]},"num_attrs":5}},"name":"day_of_week_encoded","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":2,"name":"day_of_week_encoded_Thursday"},{"idx":3,"name":"day_of_week_encoded_Tuesday"},{"idx":4,"name":"day_of_week_encoded_Wednesday"},{"idx":5,"name":"day_of_week_encoded_Monday"},{"idx":6,"name":"day_of_week_encoded_Friday"}],"numeric":[{"idx":0,"name":"UnitPrice"},{"idx":1,"name":"Quantity"}]},"num_attrs":7}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Perform some hyperparameter tuning on the model because we do not want to repeat the exact same transformations over and over again. **Caching** - puts a copy of the intermediately transformed dataset into memory. Allows repeat access it at much lower cost than running pipeline again. Run the training without caching the data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f33f395a-cd0e-4aac-b878-0803b2fd52fe"}}},{"cell_type":"code","source":["\nfrom pyspark.ml.clustering import KMeans\nkmeans = KMeans().setK(20).setSeed(1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9390ad92-bda2-452f-9bcd-45450b15d776"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["transformedTraining.cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3d361f5-312e-45fc-bae1-d30edcaf3012"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Train the model:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c8a5087-6e4c-48ed-88e2-9e15c12a3fb6"}}},{"cell_type":"code","source":["kmModel = kmeans.fit(transformedTraining)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"54b69edc-1fe5-4d70-8b36-e66da358cbdf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If we were to compute the cost according to some success merits on ourtraining set:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87af116b-664c-4a21-b811-3b5f5c40c91d"}}},{"cell_type":"code","source":["transformedTest = fittedPipeline.transform(testDataFrame)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"faf4acb9-f4e2-4aca-9a45-d5ec86398122"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"transformedTest","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"InvoiceNo","nullable":true,"type":"string"},{"metadata":{},"name":"StockCode","nullable":true,"type":"string"},{"metadata":{},"name":"Description","nullable":true,"type":"string"},{"metadata":{},"name":"Quantity","nullable":true,"type":"integer"},{"metadata":{},"name":"InvoiceDate","nullable":true,"type":"string"},{"metadata":{},"name":"UnitPrice","nullable":false,"type":"double"},{"metadata":{},"name":"CustomerID","nullable":false,"type":"double"},{"metadata":{},"name":"Country","nullable":true,"type":"string"},{"metadata":{},"name":"day_of_week","nullable":true,"type":"string"},{"metadata":{"ml_attr":{"name":"day_of_week_index","type":"nominal","vals":["Thursday","Tuesday","Wednesday","Monday","Friday","Sunday"]}},"name":"day_of_week_index","nullable":false,"type":"double"},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":0,"name":"Thursday"},{"idx":1,"name":"Tuesday"},{"idx":2,"name":"Wednesday"},{"idx":3,"name":"Monday"},{"idx":4,"name":"Friday"}]},"num_attrs":5}},"name":"day_of_week_encoded","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}},{"metadata":{"ml_attr":{"attrs":{"binary":[{"idx":2,"name":"day_of_week_encoded_Thursday"},{"idx":3,"name":"day_of_week_encoded_Tuesday"},{"idx":4,"name":"day_of_week_encoded_Wednesday"},{"idx":5,"name":"day_of_week_encoded_Monday"},{"idx":6,"name":"day_of_week_encoded_Friday"}],"numeric":[{"idx":0,"name":"UnitPrice"},{"idx":1,"name":"Quantity"}]},"num_attrs":7}},"name":"features","nullable":true,"type":{"class":"org.apache.spark.ml.linalg.VectorUDT","pyClass":"pyspark.ml.linalg.VectorUDT","sqlType":{"fields":[{"metadata":{},"name":"type","nullable":false,"type":"byte"},{"metadata":{},"name":"size","nullable":true,"type":"integer"},{"metadata":{},"name":"indices","nullable":true,"type":{"containsNull":false,"elementType":"integer","type":"array"}},{"metadata":{},"name":"values","nullable":true,"type":{"containsNull":false,"elementType":"double","type":"array"}}],"type":"struct"},"type":"udt"}}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Use RDDs to parallelize raw data that you have stored in memory on the driver machine. Parallelize some simple numbers and create a DataFrame after:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"022c0d53-013b-4051-acad-c389f4f674d7"}}},{"cell_type":"code","source":["from pyspark.sql import Row\n\nspark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d935ab9b-b583-4763-b09d-95f411834412"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[152]: DataFrame[_1: bigint]</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[152]: DataFrame[_1: bigint]</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"397a32b5-1273-4c16-a49b-2e9a6086e10d"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"A_Gentle_Introduction_to_Spark-Chapter_3_A_Tour_of_Sparks_Toolset","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1032156125227212}},"nbformat":4,"nbformat_minor":0}
