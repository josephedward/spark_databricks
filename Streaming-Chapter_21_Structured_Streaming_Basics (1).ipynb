{"cells":[{"cell_type":"markdown","source":["Streaming DataFrames are largely the same as static DataFrames. We create them within Spark applications and then perform transformations on them to get our data into the correct format.\n- Basically, all of the transformations that are available in the static Structured APIs apply to Streaming DataFrames. \n- However, one small difference is that Structured Streaming does not let you perform schema inference without explicitly enabling it. \n- You can enable schema inference for this by setting the configuration spark.sql.streaming.schemaInference to true.\n-  Given that fact, we will read the schema from one file (that we know has a valid schema) and pass the dataSchema object from our static DataFrame to our streaming DataFrame.\n-  As mentioned, you should avoid doing this in a production scenario where your data may (accidentally) change out from under you"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a632f940-fd0d-4cb6-ad88-bd69b69c55f2"}}},{"cell_type":"code","source":["static = spark.read.json(\"/FileStore/tables/activity_data\")\ndataSchema = static.schema\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7193e7f7-58e7-4782-8d3f-24c3799708ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["maxFilesPerTrigger\n- allows you to control how quickly Spark will read all of the files in the folder\n- By specifying this value lower, we’re artificially limiting the flow of the stream to one file per trigger. \n- This helps us demonstrate how Structured Streaming runs incrementally in our example, but probably isn’t something you’d use in production."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8a2ae5a-1c54-4de6-abc0-d193b290e6e4"}}},{"cell_type":"code","source":["streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1)\\\n  .json(\"/FileStore/tables/activity_data\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61dfc9e2-57e7-4fb8-9c0b-82b494c42380"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Streaming DataFrame creation and execution is lazy\n-  specify transformations on our streaming DataFrame before finally calling an action to start the stream. In this case, we’ll show one simple transformation—we will group and count data by the gt column, which is the activity being performed by the user at that point in time:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d7014d5-8bae-498f-837c-e70c44575f50"}}},{"cell_type":"code","source":["activityCounts = streaming.groupBy(\"gt\").count()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b83057f-b552-4a7c-b320-30e00e3f3b12"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["specify our action to start the query.\n-  specify an output destination, or output sink for our result of this query. \n- write to a memory sink which keeps an in-memory table of the results. \n- define how Spark will output that data. \n- use the complete output mode - rewrites all of the keys along with their counts after every trigger"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"77e3b8af-9264-48a0-9d18-6609241ac61b"}}},{"cell_type":"code","source":["activityQuery = activityCounts.writeStream.queryName(\"activity_counts\")\\\n  .format(\"memory\").outputMode(\"complete\")\\\n  .start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a20711a-e1a2-460d-94e7-004ea8565375"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from time import sleep\nfor x in range(5):\n    spark.sql(\"SELECT * FROM activity_counts\").show()\n    sleep(1)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d346534b-dfe0-45a4-b48c-1528456ec60e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------+------+\n|        gt| count|\n+----------+------+\n|  stairsup|167255|\n|       sit|196927|\n|     stand|182165|\n|      walk|212095|\n|      bike|172762|\n|stairsdown|149819|\n|      null|167168|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|  stairsup|177710|\n|       sit|209235|\n|     stand|193552|\n|      walk|225351|\n|      bike|183559|\n|stairsdown|159182|\n|      null|177614|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|  stairsup|177710|\n|       sit|209235|\n|     stand|193552|\n|      walk|225351|\n|      bike|183559|\n|stairsdown|159182|\n|      null|177614|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|  stairsup|188165|\n|       sit|221543|\n|     stand|204938|\n|      walk|238607|\n|      bike|194355|\n|stairsdown|168545|\n|      null|188061|\n+----------+------+\n\n+----------+------+\n|        gt| count|\n+----------+------+\n|  stairsup|188165|\n|       sit|221543|\n|     stand|204938|\n|      walk|238607|\n|      bike|194355|\n|stairsdown|168545|\n|      null|188061|\n+----------+------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+\n        gt| count|\n+----------+------+\n  stairsup|167255|\n       sit|196927|\n     stand|182165|\n      walk|212095|\n      bike|172762|\nstairsdown|149819|\n      null|167168|\n+----------+------+\n\n+----------+------+\n        gt| count|\n+----------+------+\n  stairsup|177710|\n       sit|209235|\n     stand|193552|\n      walk|225351|\n      bike|183559|\nstairsdown|159182|\n      null|177614|\n+----------+------+\n\n+----------+------+\n        gt| count|\n+----------+------+\n  stairsup|177710|\n       sit|209235|\n     stand|193552|\n      walk|225351|\n      bike|183559|\nstairsdown|159182|\n      null|177614|\n+----------+------+\n\n+----------+------+\n        gt| count|\n+----------+------+\n  stairsup|188165|\n       sit|221543|\n     stand|204938|\n      walk|238607|\n      bike|194355|\nstairsdown|168545|\n      null|188061|\n+----------+------+\n\n+----------+------+\n        gt| count|\n+----------+------+\n  stairsup|188165|\n       sit|221543|\n     stand|204938|\n      walk|238607|\n      bike|194355|\nstairsdown|168545|\n      null|188061|\n+----------+------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Selections and Filtering\n-  All select and filter transformations are supported in Structured Streaming\n-  as are all DataFrame functions and individual column manipulations\n- We show a simple example using selections and filtering below. \n- In this case, because we are not updating any keys over time, we will use the Append output mode, so that new results are appended to the output table:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ac6020f-1874-42fa-ad77-cf2beb1b2f15"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\nsimpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n  .where(\"stairs\")\\\n  .where(\"gt is not null\")\\\n  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n  .writeStream\\\n  .queryName(\"simple_transform\")\\\n  .format(\"memory\")\\\n  .outputMode(\"append\")\\\n  .start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d88a8d1-eef6-4b3b-91e4-d58a38f694b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Aggregations\n-  can specify arbitrary aggregations, as you saw in the Structured APIs\n- can use a more exotic aggregation, like a cube, on the phone model and activity and the average x, y, z accelerations of our sensor\n- (jump back to Chapter 7 in order to see potential aggregations that you can run on your stream)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ec21056-dbd8-4845-92fc-e02258d5e004"}}},{"cell_type":"code","source":["deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n  .drop(\"avg(Arrival_time)\")\\\n  .drop(\"avg(Creation_Time)\")\\\n  .drop(\"avg(Index)\")\\\n  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n  .outputMode(\"complete\")\\\n  .start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20938f83-b7f0-4de0-ab61-2502549c8606"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM device_counts\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3446916-97e9-41c1-8b41-b3acd046f2c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["null","nexus4",-0.007024379640278381,-5.438909838219232E-4,0.005195158657373937],[null,"nexus4",0.0011945763431121092,-0.0061174486583486214,-0.008591960711177812],[null,null,0.0011945763431121092,-0.0061174486583486214,-0.008591960711177812],["bike","nexus4",0.023853077485904806,-0.009183302863968828,-0.08251574232228004],["stand",null,-4.055901934813167E-4,4.2886781289306545E-4,2.234958592930457E-4],["sit","nexus4",-5.349253688532204E-4,3.42148544390284E-4,-1.266214101180325E-4],["stand","nexus4",-4.055901934813167E-4,4.2886781289306545E-4,2.234958592930457E-4],["stairsdown",null,0.026141112228300515,-0.03781945599450251,0.1259837445616258],["stairsup",null,-0.02687827649018481,-0.008319002048281888,-0.09788031217647133],["sit",null,-5.349253688532204E-4,3.42148544390284E-4,-1.266214101180325E-4],["stairsup","nexus4",-0.02687827649018481,-0.008319002048281888,-0.09788031217647133],["walk",null,-0.003293057926330935,0.004499464505870117,6.431841446008273E-4],["stairsdown","nexus4",0.026141112228300515,-0.03781945599450251,0.1259837445616258],["bike",null,0.023853077485904806,-0.009183302863968828,-0.08251574232228004],["walk","nexus4",-0.003293057926330935,0.004499464505870117,6.431841446008273E-4],["null",null,-0.007024379640278381,-5.438909838219232E-4,0.005195158657373937]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"gt","type":"\"string\"","metadata":"{}"},{"name":"model","type":"\"string\"","metadata":"{}"},{"name":"avg(x)","type":"\"double\"","metadata":"{}"},{"name":"avg(y)","type":"\"double\"","metadata":"{}"},{"name":"avg(z)","type":"\"double\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>gt</th><th>model</th><th>avg(x)</th><th>avg(y)</th><th>avg(z)</th></tr></thead><tbody><tr><td>null</td><td>nexus4</td><td>-0.007024379640278381</td><td>-5.438909838219232E-4</td><td>0.005195158657373937</td></tr><tr><td>null</td><td>nexus4</td><td>0.0011945763431121092</td><td>-0.0061174486583486214</td><td>-0.008591960711177812</td></tr><tr><td>null</td><td>null</td><td>0.0011945763431121092</td><td>-0.0061174486583486214</td><td>-0.008591960711177812</td></tr><tr><td>bike</td><td>nexus4</td><td>0.023853077485904806</td><td>-0.009183302863968828</td><td>-0.08251574232228004</td></tr><tr><td>stand</td><td>null</td><td>-4.055901934813167E-4</td><td>4.2886781289306545E-4</td><td>2.234958592930457E-4</td></tr><tr><td>sit</td><td>nexus4</td><td>-5.349253688532204E-4</td><td>3.42148544390284E-4</td><td>-1.266214101180325E-4</td></tr><tr><td>stand</td><td>nexus4</td><td>-4.055901934813167E-4</td><td>4.2886781289306545E-4</td><td>2.234958592930457E-4</td></tr><tr><td>stairsdown</td><td>null</td><td>0.026141112228300515</td><td>-0.03781945599450251</td><td>0.1259837445616258</td></tr><tr><td>stairsup</td><td>null</td><td>-0.02687827649018481</td><td>-0.008319002048281888</td><td>-0.09788031217647133</td></tr><tr><td>sit</td><td>null</td><td>-5.349253688532204E-4</td><td>3.42148544390284E-4</td><td>-1.266214101180325E-4</td></tr><tr><td>stairsup</td><td>nexus4</td><td>-0.02687827649018481</td><td>-0.008319002048281888</td><td>-0.09788031217647133</td></tr><tr><td>walk</td><td>null</td><td>-0.003293057926330935</td><td>0.004499464505870117</td><td>6.431841446008273E-4</td></tr><tr><td>stairsdown</td><td>nexus4</td><td>0.026141112228300515</td><td>-0.03781945599450251</td><td>0.1259837445616258</td></tr><tr><td>bike</td><td>null</td><td>0.023853077485904806</td><td>-0.009183302863968828</td><td>-0.08251574232228004</td></tr><tr><td>walk</td><td>nexus4</td><td>-0.003293057926330935</td><td>0.004499464505870117</td><td>6.431841446008273E-4</td></tr><tr><td>null</td><td>null</td><td>-0.007024379640278381</td><td>-5.438909838219232E-4</td><td>0.005195158657373937</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["historicalAgg = static.groupBy(\"gt\", \"model\").avg()\ndeviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n  .cube(\"gt\", \"model\").avg()\\\n  .join(historicalAgg, [\"gt\", \"model\"])\\\n  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n  .outputMode(\"complete\")\\\n  .start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73bccfc0-b893-4046-a733-55eadebcf836"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1752595641097406&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>historicalAgg <span class=\"ansi-blue-fg\">=</span> static<span class=\"ansi-blue-fg\">.</span>groupBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>avg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> deviceModelStats <span class=\"ansi-blue-fg\">=</span> streaming<span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Arrival_Time&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;Creation_Time&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;Index&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   <span class=\"ansi-blue-fg\">.</span>cube<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>avg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>historicalAgg<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   <span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>queryName<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;device_counts&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;memory&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;static&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;static&#39; is not defined","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1752595641097406&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>historicalAgg <span class=\"ansi-blue-fg\">=</span> static<span class=\"ansi-blue-fg\">.</span>groupBy<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>avg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> deviceModelStats <span class=\"ansi-blue-fg\">=</span> streaming<span class=\"ansi-blue-fg\">.</span>drop<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Arrival_Time&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;Creation_Time&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;Index&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>   <span class=\"ansi-blue-fg\">.</span>cube<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>avg<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>   <span class=\"ansi-blue-fg\">.</span>join<span class=\"ansi-blue-fg\">(</span>historicalAgg<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#34;gt&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;model&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span>   <span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>queryName<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;device_counts&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;memory&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;static&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Reading from the Kafka Source\n- To read, you first need to choose one of the following options: assign, subscribe, or subscribePattern\n- Only one of these can be present as an option when you go to read from Kafka.\n- Assign is a fine-grained way of specifying not just the topic but also the topic partitions from which you would like to read. \n- This is specified as a JSON string {\"topicA\": [0,1],\"topicB\":[2,4]}. subscribe and subscribePattern are ways of subscribing to one or more topics either by specifying a list of topics (in the former) or via a pattern (via the latter).\n- Second, you will need to specify the kafka.bootstrap.servers that Kafka provides to connect to the service."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a90f587f-8296-47cf-979f-efb835ad50ec"}}},{"cell_type":"code","source":["# Subscribe to 1 topic\ndf1 = spark.readStream.format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n  .option(\"subscribe\", \"topic1\")\\\n  .load()\n# Subscribe to multiple topics\ndf2 = spark.readStream.format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n  .option(\"subscribe\", \"topic1,topic2\")\\\n  .load()\n# Subscribe to a pattern\ndf3 = spark.readStream.format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n  .option(\"subscribePattern\", \"topic.*\")\\\n  .load()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"409a4252-cd38-4ee9-84eb-6017e7ddb07c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Writing to the Kafka Sink\n- Writing to Kafka queries is largely the same as reading from them except for fewer parameters.\n- You’ll still need to specify the Kafka bootstrap servers, but the only other option you will need to supply is either a column with the topic specification or supply that as an option."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93dee428-9e93-4d43-affa-2cb264e7216f"}}},{"cell_type":"code","source":["df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n  .writeStream\\\n  .format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n  .start()\ndf1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n  .writeStream\\\n  .format(\"kafka\")\\\n  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n  .option(\"topic\", \"topic1\")\\\n  .start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44de83cd-bb55-49e4-95c8-49ffc508d38b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 3331, in run_code\n  File &#34;&lt;command-1752595641097408&gt;&#34;, line 5, in &lt;module&gt;\n    .option(&#34;checkpointLocation&#34;, &#34;/to/HDFS-compatible/dir&#34;)\\\n  File &#34;/databricks/spark/python/pyspark/sql/streaming.py&#34;, line 1225, in start\n  File &#34;/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py&#34;, line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &#34;/databricks/spark/python/pyspark/sql/utils.py&#34;, line 127, in deco\n  File &#34;/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py&#34;, line 328, in get_return_value\n    format(target_id, &#34;.&#34;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o530.start.\n: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:974)\n\tat com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:4099)\n\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3386)\n\tat org.json4s.jackson.Serialization$.write(Serialization.scala:27)\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:80)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:182)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.&lt;init&gt;(StreamExecution.scala:180)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:52)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:327)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:369)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:390)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:317)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:311)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:308)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:307)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:545)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:543)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:146)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:560)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:37)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\t\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\t\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\t\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:974)\n\t\tat com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:4099)\n\t\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3386)\n\t\tat org.json4s.jackson.Serialization$.write(Serialization.scala:27)\n\t\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:80)\n\t\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:182)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.sql.execution.streaming.StreamExecution.&lt;init&gt;(StreamExecution.scala:180)\n\t\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:52)\n\t\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:327)\n\t\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:369)\n\t\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:390)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\t\tat py4j.Gateway.invoke(Gateway.java:295)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\t\t... 1 more\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 2044, in showtraceback\nAttributeError: &#39;Py4JJavaError&#39; object has no attribute &#39;_render_traceback_&#39;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 1151, in get_records\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 319, in wrapped\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 353, in _fixed_getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1502, in getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1460, in getframeinfo\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 696, in getsourcefile\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 725, in getmodule\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 709, in getabsfile\n  File &#34;/local_disk0/pythonVirtualEnvDirs/virtualEnv-977b0a77-1b3f-416b-ab28-ab489d3a1acc/lib/python3.7/posixpath.py&#34;, line 383, in abspath\nFileNotFoundError: [Errno 2] No such file or directory\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 3331, in run_code\n  File &#34;&lt;command-1752595641097408&gt;&#34;, line 5, in &lt;module&gt;\n    .option(&#34;checkpointLocation&#34;, &#34;/to/HDFS-compatible/dir&#34;)\\\n  File &#34;/databricks/spark/python/pyspark/sql/streaming.py&#34;, line 1225, in start\n  File &#34;/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py&#34;, line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File &#34;/databricks/spark/python/pyspark/sql/utils.py&#34;, line 127, in deco\n  File &#34;/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py&#34;, line 328, in get_return_value\n    format(target_id, &#34;.&#34;, name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o530.start.\n: java.rmi.RemoteException: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053); nested exception is: \n\tcom.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:128)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:974)\n\tat com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:4099)\n\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3386)\n\tat org.json4s.jackson.Serialization$.write(Serialization.scala:27)\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:80)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:182)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.&lt;init&gt;(StreamExecution.scala:180)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:52)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:327)\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:369)\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:390)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.databricks.api.base.DatabricksServiceException: QUOTA_EXCEEDED: You have exceeded the maximum number of allowed files on Databricks Community Edition. To ensure free access, you are limited to 10000 files and 10 GB of storage in DBFS. Please use dbutils.fs to list and clean up files to restore service. You may have to wait a few minutes after cleaning up the files for the quota to be refreshed. (Files found: 15053)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:317)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:311)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException(ScalaProto2RpcSerializer.scala:308)\n\tat com.databricks.api.rpc.BaseScalaProtoRpcSerializer.deserializeException$(ScalaProto2RpcSerializer.scala:307)\n\tat com.databricks.api.rpc.ScalaProto2CodeGeneratedRpcSerializer.deserializeException(ScalaProto2CodeGeneratedRpcSerializer.scala:34)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.$anonfun$onComplete$4(Jetty9Client.scala:545)\n\tat com.databricks.util.UntrustedUtils$.logUncaughtExceptions(UntrustedUtils.scala:36)\n\tat com.databricks.rpc.Jetty9Client$$anon$1.onComplete(Jetty9Client.scala:543)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:198)\n\tat shaded.v9_4.org.eclipse.jetty.client.ResponseNotifier.notifyComplete(ResponseNotifier.java:190)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.terminateResponse(HttpReceiver.java:444)\n\tat shaded.v9_4.org.eclipse.jetty.client.HttpReceiver.responseSuccess(HttpReceiver.java:390)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.messageComplete(HttpReceiverOverHTTP.java:316)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.handleContentMessage(HttpParser.java:574)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseContent(HttpParser.java:1644)\n\tat shaded.v9_4.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:1490)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.parse(HttpReceiverOverHTTP.java:172)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.process(HttpReceiverOverHTTP.java:135)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpReceiverOverHTTP.receive(HttpReceiverOverHTTP.java:73)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpChannelOverHTTP.receive(HttpChannelOverHTTP.java:133)\n\tat shaded.v9_4.org.eclipse.jetty.client.http.HttpConnectionOverHTTP.onFillable(HttpConnectionOverHTTP.java:151)\n\tat shaded.v9_4.org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:426)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:320)\n\tat shaded.v9_4.org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:158)\n\tat shaded.v9_4.org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103)\n\tat shaded.v9_4.org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:336)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:313)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:171)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:129)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:367)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:782)\n\tat shaded.v9_4.org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:914)\n\t... 1 more\n\tSuppressed: com.databricks.rpc.Jetty9Client$Jetty9ClientException: Exception in send()\n\t\tat com.databricks.rpc.Jetty9Client.send(Jetty9Client.scala:146)\n\t\tat com.databricks.rpc.DynamicJettyClient.send(BaseJettyClient.scala:560)\n\t\tat com.databricks.rpc.BoundRPCClient.send(BoundRPCClient.scala:37)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.$anonfun$doSend$3(DbfsClient.scala:166)\n\t\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:239)\n\t\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:234)\n\t\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:231)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionContext(DbfsClient.scala:28)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:276)\n\t\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:269)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.withAttributionTags(DbfsClient.scala:28)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.doSend(DbfsClient.scala:152)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:103)\n\t\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIgnoreDraining(DbfsClient.scala:88)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close0(DbfsOutputStream.scala:100)\n\t\tat com.databricks.backend.daemon.data.client.DbfsOutputStream.close(DbfsOutputStream.scala:79)\n\t\tat java.io.FilterOutputStream.close(FilterOutputStream.java:159)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)\n\t\tat org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)\n\t\tat sun.nio.cs.StreamEncoder.implClose(StreamEncoder.java:320)\n\t\tat sun.nio.cs.StreamEncoder.close(StreamEncoder.java:149)\n\t\tat java.io.OutputStreamWriter.close(OutputStreamWriter.java:233)\n\t\tat com.fasterxml.jackson.core.json.WriterBasedJsonGenerator.close(WriterBasedJsonGenerator.java:974)\n\t\tat com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:4099)\n\t\tat com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3386)\n\t\tat org.json4s.jackson.Serialization$.write(Serialization.scala:27)\n\t\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:80)\n\t\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:182)\n\t\tat scala.Option.getOrElse(Option.scala:189)\n\t\tat org.apache.spark.sql.execution.streaming.StreamExecution.&lt;init&gt;(StreamExecution.scala:180)\n\t\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.&lt;init&gt;(MicroBatchExecution.scala:52)\n\t\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:327)\n\t\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:369)\n\t\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:390)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\t\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\t\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\t\tat java.lang.reflect.Method.invoke(Method.java:498)\n\t\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\t\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\t\tat py4j.Gateway.invoke(Gateway.java:295)\n\t\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\t\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\t\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\t\t... 1 more\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 2044, in showtraceback\nAttributeError: &#39;Py4JJavaError&#39; object has no attribute &#39;_render_traceback_&#39;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 1151, in get_records\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 319, in wrapped\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 353, in _fixed_getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1502, in getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1460, in getframeinfo\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 696, in getsourcefile\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 725, in getmodule\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 709, in getabsfile\n  File &#34;/local_disk0/pythonVirtualEnvDirs/virtualEnv-977b0a77-1b3f-416b-ab28-ab489d3a1acc/lib/python3.7/posixpath.py&#34;, line 383, in abspath\nFileNotFoundError: [Errno 2] No such file or directory\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span></div>","errorSummary":"Unknown error.","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Socket source\n- The socket source allows you to send data to your Streams via TCP sockets. \n- To start one, specify a host and port to read data from. \n- Spark will open a new TCP connection to read from that address. \n- The socket source should not be used in production because the socket sits on the driver and does not provide end-to-end fault-tolerance guarantees. \n- Here is a short example of setting up this source to read from localhost:9999:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"33ab5759-24da-4ba5-9068-67a9573cac23"}}},{"cell_type":"code","source":["socketDF = spark.readStream.format(\"socket\")\\\n  .option(\"host\", \"localhost\").option(\"port\", 9999).load()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"747413e4-d44c-41d6-a148-0f47be9d4791"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Console sink \n- The console sink allows you to write out some of your streaming query to the console. \n- This is useful for debugging but is not fault-tolerant.\n-  Writing out to the console is simple and only prints some rows of your streaming query to the console. This supports both append and complete output modes:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70278c5c-a40b-42b8-a3f2-b6cc101ee584"}}},{"cell_type":"code","source":["activityCounts.writeStream.trigger(processingTime='5 seconds')\\\n  .format(\"console\").outputMode(\"complete\").start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16d7db16-3215-4652-90d7-34f65b58c7a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1752595641097410&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>activityCounts<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>trigger<span class=\"ansi-blue-fg\">(</span>processingTime<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;5 seconds&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;console&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;complete&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;activityCounts&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;activityCounts&#39; is not defined","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-1752595641097410&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>activityCounts<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>trigger<span class=\"ansi-blue-fg\">(</span>processingTime<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;5 seconds&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\">\\</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>   <span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;console&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;complete&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;activityCounts&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Once trigger\n- You can also just run a streaming job once by setting that as the trigger. \n- This might seem like a weird case, but it’s actually extremely useful in both development and production. \n- During development, you can test your application on just one trigger’s worth of data at a time. \n- During production, the Once trigger can be used to run your job manually at a low rate (e.g., import new data into a summary table just occasionally). \n- Because Structured Streaming still fully tracks all the input files processed and the state of the computation, this is easier than writing your own custom logic to track this in a batch job, and saves a lot of resources over running a continuous job 24/7:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8698cec4-dece-48b4-b69e-5392da371de9"}}},{"cell_type":"code","source":["activityCounts.writeStream.trigger(once=True)\\\n  .format(\"console\").outputMode(\"complete\").start()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5344e691-f096-4820-be8e-7e1eefae27c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 3331, in run_code\n  File &#34;&lt;command-1752595641097411&gt;&#34;, line 1, in &lt;module&gt;\n    activityCounts.writeStream.trigger(once=True)\\\nNameError: name &#39;activityCounts&#39; is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 2044, in showtraceback\nAttributeError: &#39;NameError&#39; object has no attribute &#39;_render_traceback_&#39;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 1151, in get_records\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 319, in wrapped\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 353, in _fixed_getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1502, in getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1460, in getframeinfo\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 696, in getsourcefile\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 725, in getmodule\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 709, in getabsfile\n  File &#34;/local_disk0/pythonVirtualEnvDirs/virtualEnv-977b0a77-1b3f-416b-ab28-ab489d3a1acc/lib/python3.7/posixpath.py&#34;, line 383, in abspath\nFileNotFoundError: [Errno 2] No such file or directory\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">ERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 3331, in run_code\n  File &#34;&lt;command-1752595641097411&gt;&#34;, line 1, in &lt;module&gt;\n    activityCounts.writeStream.trigger(once=True)\\\nNameError: name &#39;activityCounts&#39; is not defined\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/interactiveshell.py&#34;, line 2044, in showtraceback\nAttributeError: &#39;NameError&#39; object has no attribute &#39;_render_traceback_&#39;\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 1151, in get_records\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 319, in wrapped\n  File &#34;/databricks/python/lib/python3.7/site-packages/IPython/core/ultratb.py&#34;, line 353, in _fixed_getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1502, in getinnerframes\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 1460, in getframeinfo\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 696, in getsourcefile\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 725, in getmodule\n  File &#34;/usr/lib/python3.7/inspect.py&#34;, line 709, in getabsfile\n  File &#34;/local_disk0/pythonVirtualEnvDirs/virtualEnv-977b0a77-1b3f-416b-ab28-ab489d3a1acc/lib/python3.7/posixpath.py&#34;, line 383, in abspath\nFileNotFoundError: [Errno 2] No such file or directory\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span></div>","errorSummary":"Unknown error.","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0da2abf0-3021-4a3d-be9a-4c0b12270392"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Streaming-Chapter_21_Structured_Streaming_Basics","dashboards":[],"language":"python","widgets":{},"notebookOrigID":1752595641097398}},"nbformat":4,"nbformat_minor":0}
